# NodeAva

A self-contained digital avatar that thinks, speaks, and listens — running entirely on your machine.

## A Quick Note

I built this for myself as a proof of concept. It's a starting point, not a polished product. I'm sharing it because it was fun to build and maybe useful to others exploring the same space. I'm not looking to heavily maintain this — fork it, break it, make it your own. Good vibes to you! :v:

## What It Does

NodeAva is a 3D talking avatar powered entirely by local AI models:

- **Thinks** before responding using a reasoning model (Qwen3-4B)
- **Speaks** with natural voice synthesis (Kokoro TTS)
- **Listens** with in-browser voice activity detection + speech-to-text (Whisper)
- **Emotes** with facial expressions and lip sync matched to speech
- **Runs locally** — no cloud APIs, no data leaves your machine

## Requirements

- **GPU**: NVIDIA or AMD with **8 GB+ VRAM** (GPU is required, no CPU-only mode)
- **Docker**: Docker Engine with Compose V2
- **Disk**: ~5 GB for models + ~10 GB for Docker images (AMD: ~30 GB due to ROCm)
- **OS**: Linux or Windows (with Docker Desktop)

## Getting an Avatar

NodeAva doesn't ship with an avatar model for licensing reasons. You need to provide a GLB file with ARKit blend shapes and Oculus Visemes.

**Quickest path:**
1. Download [VRoid Studio](https://vroid.com/en/studio) (free)
2. Create a character
3. Export as VRM (TalkingHead supports VRM natively)
4. Save as `frontend/public/avatars/default-avatar.glb`

See `frontend/public/avatars/README.md` for more options.

## Quick Start

```bash
# 1. Clone
git clone https://github.com/YourUser/nodeava.git
cd nodeava

# 2. Run setup (detects GPU, downloads model, generates .env)
./scripts/setup.sh          # Linux
# .\scripts\setup.ps1       # Windows PowerShell

# 3. Start (use the command printed by setup)
docker compose -f docker-compose.yml -f docker-compose.gpu-nvidia.yml up --build
```

Open **http://localhost:3000** — type or speak to Ava.

## GPU Support

| GPU | Linux | Windows |
|-----|-------|---------|
| **NVIDIA** | Full support (CUDA) | Full support (Docker Desktop) |
| **AMD** | Full support (Vulkan + ROCm) | Not supported* |

*Docker Desktop on Windows uses WSL2 which only supports NVIDIA GPU passthrough.

### Compose Commands

| Setup | Command |
|-------|---------|
| Linux/Windows + NVIDIA | `docker compose -f docker-compose.yml -f docker-compose.gpu-nvidia.yml up --build` |
| Linux + AMD | `docker compose -f docker-compose.yml -f docker-compose.gpu-amd.yml up --build` |

> **AMD first build**: The ROCm TTS image builds from source (~20-30 min). Subsequent builds use Docker cache.

## Architecture

```
Browser (localhost:3000)
  └── nginx ──┬── /api/stt/ ──► whisper.cpp  (Vulkan,  port 8080)
              ├── /api/tts/ ──► Kokoro-FastAPI (CUDA/ROCm, port 8880)
              └── /api/llm/ ──► llama.cpp    (CUDA/Vulkan, port 8081)
```

| Component | Model | Size | Purpose |
|-----------|-------|------|---------|
| **LLM** | Qwen3-4B Q4_K_M | 2.5 GB | Conversational AI with reasoning |
| **TTS** | Kokoro-82M | ~330 MB | Text-to-speech with word timestamps |
| **STT** | Whisper base.en | 142 MB | Speech-to-text (auto-downloaded) |
| **Avatar** | TalkingHead | User-provided GLB | 3D avatar with lip sync + emotions |

The browser is the orchestrator — it streams LLM tokens, splits at sentence boundaries, sends each sentence to TTS immediately, and drives the avatar's lip sync and emotions. No backend orchestrator needed.

### Thinking Mode

Ava uses Qwen3's thinking mode: the model reasons internally before responding, producing better quality answers. The thinking content (`<think>...</think>`) is automatically filtered — you only see the final response.

### VRAM Usage (~4.8 GB total)

| Component | VRAM |
|-----------|------|
| LLM (Qwen3-4B Q4_K_M, 4096 ctx) | ~3.5 GB |
| TTS (Kokoro-82M) | ~1.0 GB |
| STT (Whisper base.en) | ~0.3 GB |

## Configuration

All settings are in `.env` (generated by setup script):

| Variable | Default | Description |
|----------|---------|-------------|
| `GPU_TYPE` | `nvidia` | GPU vendor (`nvidia` or `amd`) |
| `FRONTEND_PORT` | `3000` | Browser access port |
| `LLM_PORT` | `8081` | LLM API port |
| `TTS_PORT` | `8880` | TTS API port |
| `STT_PORT` | `8080` | STT API port |
| `LLM_MODEL_FILE` | `Qwen_Qwen3-4B-Instruct-2507-Q4_K_M.gguf` | LLM model filename |
| `LLM_CTX_SIZE` | `4096` | LLM context window |
| `WHISPER_MODEL` | `base.en` | Whisper model size |

## Troubleshooting

**"No GPU detected"**
- NVIDIA: Install the [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html)
- AMD: Ensure `/dev/kfd` and `/dev/dri` exist. Install ROCm kernel driver.

**LLM not responding**
- Wait ~30s after startup for model loading
- Check: `curl http://localhost:8081/v1/models`
- Check logs: `docker compose logs llm`

**No audio / lip sync**
- Browser requires user interaction before playing audio (click anywhere first)
- Check TTS: `docker compose logs tts`

**AMD TTS build fails**
- Ensure you have ~30 GB free disk space for the ROCm image
- The first build downloads ~15 GB of ROCm packages — needs stable internet

**Avatar not loading**
- Ensure `frontend/public/avatars/default-avatar.glb` exists
- Must have ARKit blend shapes and Oculus Visemes

## License

Apache-2.0 — see [LICENSE](LICENSE) and [THIRD-PARTY-LICENSES.md](THIRD-PARTY-LICENSES.md).
