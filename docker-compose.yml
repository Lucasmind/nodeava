services:
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - "${FRONTEND_PORT:-3000}:80"
    depends_on:
      stt:
        condition: service_healthy
      tts:
        condition: service_healthy
      llm:
        condition: service_healthy
    restart: unless-stopped

  llm:
    image: ghcr.io/ggml-org/llama.cpp:server
    volumes:
      - ./models:/models:ro
    ports:
      - "${LLM_PORT:-8081}:8080"
    command: >
      -m /models/${LLM_MODEL_FILE:-Qwen_Qwen3-4B-Instruct-2507-Q4_K_M.gguf}
      --host 0.0.0.0 --port 8080
      --ctx-size ${LLM_CTX_SIZE:-4096}
      --jinja --reasoning-format none
      --temp 0.6 --top-k 20 --top-p 0.95 --min-p 0
      --n-gpu-layers 0
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/v1/models"]
      interval: 5s
      timeout: 3s
      retries: 30
      start_period: 30s
    restart: unless-stopped

  tts:
    image: ghcr.io/remsky/kokoro-fastapi-gpu:v0.2.4
    ports:
      - "${TTS_PORT:-8880}:8880"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8880/v1/models"]
      interval: 5s
      timeout: 3s
      retries: 30
      start_period: 30s
    restart: unless-stopped

  stt:
    build:
      context: ./stt-service
      dockerfile: Dockerfile
    volumes:
      - ./models:/models
    environment:
      - WHISPER_MODEL=${WHISPER_MODEL:-base.en}
      - STT_PORT=8080
    ports:
      - "${STT_PORT:-8080}:8080"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/"]
      interval: 5s
      timeout: 3s
      retries: 30
      start_period: 30s
    restart: unless-stopped
